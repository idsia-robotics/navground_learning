{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef53465f-661a-4f55-b3f7-76e96ddb3c3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Navground-PettingZoo integration \n",
    "\n",
    "This notebook showcases the integration between navground and PettingZoo, the \"multi-agent\" version of Gymnasium.\n",
    "We focus on the differences compared to with Gymnasium: have a look at `Navground-Gymnasium integration` for the common parts (e.g., rendering).\n",
    "While with Gymnasium, we control a single navground agent (which may move among many other agents controlled by navground), with PettingZoo we can control multiple agents, even all the agents of a navground simulation. We load the same scenario with 20 agents and the same sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec8665a-e9d2-4d40-8a4c-486c34aa2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground import sim\n",
    "import numpy as np\n",
    "\n",
    "scenario = sim.load_scenario(\"\"\"\n",
    "type: Cross\n",
    "agent_margin: 0.1\n",
    "side: 4\n",
    "target_margin: 0.1\n",
    "tolerance: 0.5\n",
    "groups:\n",
    "  -\n",
    "    type: thymio\n",
    "    number: 20\n",
    "    radius: 0.1\n",
    "    control_period: 0.1\n",
    "    speed_tolerance: 0.02\n",
    "    color: gray\n",
    "    kinematics:\n",
    "      type: 2WDiff\n",
    "      wheel_axis: 0.094\n",
    "      max_speed: 0.12\n",
    "    behavior:\n",
    "      type: HL\n",
    "      optimal_speed: 0.12\n",
    "      horizon: 5.0\n",
    "      tau: 0.25\n",
    "      eta: 0.5\n",
    "      safety_margin: 0.1\n",
    "    state_estimation:\n",
    "      type: Bounded\n",
    "      range: 5.0\n",
    "\"\"\")\n",
    "\n",
    "sensor = sim.load_state_estimation(\"\"\"\n",
    "type: Discs\n",
    "number: 5\n",
    "range: 5.0\n",
    "max_speed: 0.12\n",
    "max_radius: 0.1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63230493-dffb-4a8f-be33-99b93f746ab0",
   "metadata": {},
   "source": [
    "## A single group\n",
    "\n",
    "Now, instead of a single agent, we want to control a group of agents with a policy acting on the selected sensor.\n",
    "We define the PettingZoo environment, controlling the first 10 agents, *sharing* the same configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc2b1e2-bb71-4153-9cff-a92056e5cbbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from navground_learning.env.pz import shared_parallel_env\n",
    "from navground_learning.reward import SocialReward\n",
    "from navground_learning import ObservationConfig, ControlActionConfig\n",
    "\n",
    "observation_config = ObservationConfig()\n",
    "action_config = ControlActionConfig()\n",
    "\n",
    "env = shared_parallel_env(\n",
    "    scenario=scenario,\n",
    "    agent_indices=slice(0, 10, 1),\n",
    "    sensor=sensor,\n",
    "    action=action_config,\n",
    "    observation=observation_config,\n",
    "    reward=SocialReward(),\n",
    "    time_step=0.1,\n",
    "    max_duration=60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1204acf-0024-47b0-b0ff-9ea89608951c",
   "metadata": {},
   "source": [
    "All agents have the same observation and action spaces has configured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446a4763-8da6-4622-8d99-44204a5b5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are controlling 10 agents\n",
      "They share the same observation Dict('position': Box(-5.0, 5.0, (5, 2), float64), 'radius': Box(0.0, 0.1, (5,), float64), 'valid': MultiBinary((5,)), 'velocity': Box(-0.12, 0.12, (5, 2), float64), 'ego_target_direction': Box(-1.0, 1.0, (2,), float64), 'ego_target_distance': Box(0.0, 5.0, (1,), float64)) and action Box(-1.0, 1.0, (2,), float64) spaces\n"
     ]
    }
   ],
   "source": [
    "print(f'We are controlling {len(env.possible_agents)} agents')\n",
    "\n",
    "observation_space = env.observation_space(0)\n",
    "action_space = env.action_space(0) \n",
    "if all(env.action_space(i) ==  action_space and env.observation_space(i) == observation_space \n",
    "       for i in env.possible_agents):\n",
    "    print(f'They share the same observation {observation_space} and action {action_space} spaces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a91ba-0043-4436-884c-670d67d87e2e",
   "metadata": {},
   "source": [
    "The `info` map returned by `reset(...)` and `step(...)` contains the action computed by original navground behavior, in this case `HL`, for each of the 10 agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9750f9c3-a422-42c3-b06c-02bbedb02865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation #0: {'position': array([[ 0.10201398, -0.28317614],\n",
      "       [-0.20867327, -0.35694126],\n",
      "       [ 0.19885384, -0.67285531],\n",
      "       [-0.56413084, -0.50433907],\n",
      "       [-0.82594673, -0.22419791]]), 'radius': array([0.1, 0.1, 0.1, 0.1, 0.1]), 'valid': array([1, 1, 1, 1, 1], dtype=uint8), 'velocity': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]), 'ego_target_direction': array([ 1.00000000e+00, -3.41599925e-17]), 'ego_target_distance': array([2.1121034])}\n",
      "Info #0: {'navground_action': array([0.32967995, 0.        ])}\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "print(f\"Observation #0: {observations[0]}\")\n",
    "print(f\"Info #0: {infos[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d921c1f-6a65-4bc0-8ff9-e0e771a5e53e",
   "metadata": {},
   "source": [
    "Let's collect the reward from the original controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f46e603-500d-4d16-bd7d-b939209376f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 599 steps\n",
      "mean reward -0.242\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: info['navground_action'] for i, info in infos.items()}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5127035-c9df-4345-86a1-3463eabb05df",
   "metadata": {},
   "source": [
    "and compare it with the reward from a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b2a2b3-d5c3-4146-bb0d-ddebbc55d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 599 steps\n",
      "mean reward -1.017\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "all_rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: env.action_space(i).sample() for i in range(10)}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa6665-82bb-441f-b54f-1f0d25afded3",
   "metadata": {},
   "source": [
    "We want to use a machine learning policy to generate to action. For instance, a random policy, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e144683-6316-4e64-83fd-781ed15cc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.policies.base import RandomPolicy\n",
    "\n",
    "policies = {i: RandomPolicy(env.observation_space(i), env.action_space(i)) for i in env.agents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f31954-63c4-4fa9-82c8-a27611037230",
   "metadata": {},
   "source": [
    "Policies output a tuple `(action, state)`. Therefore the new loop is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303b7779-e50b-441b-9554-66a72adfaee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 599 steps\n",
      "mean reward -1.073\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: policies[i].predict(observations[i])[0] for i in env.agents}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49992c5-64da-4860-9bb3-a7ba9a203e09",
   "metadata": {},
   "source": [
    "## Two groups\n",
    "\n",
    "Let us now consider the more complex case where we want to control agents using different sensors and/or configurations.\n",
    "For instance, we want to control the first 10 agents like before and the second 10 agents using a lidar scanner.\n",
    "Let say we also want to control the second group in acceleration vs the first group in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d760369a-cf39-47ba-ad8a-cb8272c6340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar = sim.load_state_estimation(\"\"\"\n",
    "type: Lidar\n",
    "resolution: 100\n",
    "range: 5.0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121f6736-2017-4e8c-a5fd-35a3dc212ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground_learning.env.pz import parallel_env\n",
    "from navground_learning import WorldConfig, GroupConfig\n",
    "\n",
    "first_group = GroupConfig(indices=slice(0, 10, 1), sensor=sensor, observation = ObservationConfig(include_target_distance=False), \n",
    "                          action = ControlActionConfig(), reward=SocialReward())\n",
    "second_group = GroupConfig(indices=slice(10, 20, 1), sensor=lidar, observation = ObservationConfig(), \n",
    "                           action = ControlActionConfig(use_acceleration_action=True, max_acceleration=1.0, max_angular_acceleration=10.0), reward=SocialReward())\n",
    "\n",
    "env = parallel_env(scenario=scenario, config=WorldConfig(groups=[first_group, second_group]), \n",
    "                   time_step=0.1, max_duration=60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff36a0-f985-43a9-af3c-13e3b530f545",
   "metadata": {},
   "source": [
    "The two groups uses now different observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae72065e-0234-48e8-98e4-fabb0ac055a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('position': Box(-5.0, 5.0, (5, 2), float64), 'radius': Box(0.0, 0.1, (5,), float64), 'valid': MultiBinary((5,)), 'velocity': Box(-0.12, 0.12, (5, 2), float64), 'ego_target_direction': Box(-1.0, 1.0, (2,), float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ce4ff0-6159-4983-9bec-df4da1a6d491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('range': Box(0.0, 5.0, (100,), float64), 'ego_target_direction': Box(-1.0, 1.0, (2,), float64), 'ego_target_distance': Box(0.0, 5.0, (1,), float64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8adab-148f-4aac-bfe5-2135628cc506",
   "metadata": {},
   "source": [
    "and differnet maps between actions and commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a05288-55ad-4708-9635-20511c5d637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twist2((0.120000, 0.000000), 2.553191, frame=Frame.relative)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._possible_agents[0].gym.get_cmd_from_action(np.ones(2), time_step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d27102a-fd11-45f2-bb80-24f3c3aa3963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twist2((0.100000, 0.000000), 1.000000, frame=Frame.relative)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._possible_agents[10].gym.get_cmd_from_action(np.ones(2), time_step=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cde0d0-ee7c-4d72-8bc3-c0f4d564a33f",
   "metadata": {},
   "source": [
    "## Convert to a Gymnasium Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5bfd0-c46e-4417-9c2d-2c1e834bfb57",
   "metadata": {},
   "source": [
    "In case the agents share the same configuration (and in particular action and observation spaces), we can convert the PettingZoo env in a Gymnasium vector env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ecbf625-7ba7-48a7-b713-a105c5d7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = shared_parallel_env(\n",
    "    scenario=scenario,\n",
    "    agent_indices=slice(0, 10, 1),\n",
    "    sensor=sensor,\n",
    "    action=action_config,\n",
    "    observation=observation_config,\n",
    "    reward=SocialReward(),\n",
    "    time_step=0.1,\n",
    "    max_duration=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b2ce65-fadb-4810-b1d6-a7c55bd2efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supersuit\n",
    "\n",
    "venv = supersuit.pettingzoo_env_to_vec_env_v1(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d272656-4212-4519-b79b-2b134cab1222",
   "metadata": {},
   "source": [
    "with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a44418c4-86e2-4a70-aa15-2932ea9e230d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd81c8a-7cf5-4b87-b9e1-bb4665c64c86",
   "metadata": {},
   "source": [
    "environments that represents the individual agents. \n",
    "\n",
    "This vector env follows the Gymnasium API, stacking together observation, actions of the individual agents\n",
    "\n",
    "If we want instead an vector env to follows the SB3 API, we can use (even stacking multiple vectorized envs together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080562c4-1d42-4bbc-aaf7-dda7c824f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv1 = supersuit.concat_vec_envs_v1(venv, 2, num_cpus=1, base_class=\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7f2bcd3-26ab-40f5-b885-2bb5630feae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv1.num_envs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
