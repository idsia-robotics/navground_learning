{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef53465f-661a-4f55-b3f7-76e96ddb3c3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Navground-PettingZoo integration \n",
    "\n",
    "This notebook showcases the integration between navground and PettingZoo, the \"multi-agent\" version of Gymnasium.\n",
    "We focus on the differences compared to with Gymnasium: have a look at [the Gymnasium notebook](Gymnasium.ipynb) for the common parts (e.g., rendering).\n",
    "\n",
    "While in Gymnasium we control a single navground agent (which may move among many other agents controlled by navground), with PettingZoo we can control multiple agents, even all the agents of a navground simulation. \n",
    "\n",
    "To start, we load the same scenario with 20 agents and the same sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec8665a-e9d2-4d40-8a4c-486c34aa2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground import sim\n",
    "import numpy as np\n",
    "\n",
    "from navground import sim\n",
    "\n",
    "with open('scenario.yaml') as f:\n",
    "    scenario = sim.load_scenario(f.read())\n",
    "\n",
    "with open('sensor.yaml') as f:\n",
    "    sensor = sim.load_state_estimation(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63230493-dffb-4a8f-be33-99b93f746ab0",
   "metadata": {},
   "source": [
    "## A single group\n",
    "\n",
    "Now, instead of a single agent, we want to control a group of agents with a policy acting on the selected sensor.\n",
    "We define the PettingZoo environment, controlling the first 10 agents, *sharing* the same configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc2b1e2-bb71-4153-9cff-a92056e5cbbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from navground.learning.parallel_env import shared_parallel_env\n",
    "from navground.learning import DefaultObservationConfig, ControlActionConfig\n",
    "from navground.learning.rewards import SocialReward\n",
    "\n",
    "observation_config = DefaultObservationConfig(include_target_direction=True, \n",
    "                                              include_target_distance=True)\n",
    "action_config = ControlActionConfig()\n",
    "\n",
    "env = shared_parallel_env(\n",
    "    scenario=scenario,\n",
    "    indices=slice(0, 10, 1),\n",
    "    sensor=sensor,\n",
    "    action=action_config,\n",
    "    observation=observation_config,\n",
    "    reward=SocialReward(),\n",
    "    time_step=0.1,\n",
    "    max_duration=60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1204acf-0024-47b0-b0ff-9ea89608951c",
   "metadata": {},
   "source": [
    "All agents have the same observation and action spaces has configured "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446a4763-8da6-4622-8d99-44204a5b5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are controlling 10 agents\n",
      "They share the same observation Dict('position': Box(-5.0, 5.0, (5, 2), float32), 'radius': Box(0.0, 0.1, (5,), float32), 'valid': Box(0, 1, (5,), uint8), 'velocity': Box(-0.12, 0.12, (5, 2), float32), 'ego_target_direction': Box(-1.0, 1.0, (2,), float32), 'ego_target_distance': Box(0.0, inf, (1,), float32)) and action Box(-1.0, 1.0, (2,), float32) spaces\n"
     ]
    }
   ],
   "source": [
    "print(f'We are controlling {len(env.possible_agents)} agents')\n",
    "\n",
    "observation_space = env.observation_space(0)\n",
    "action_space = env.action_space(0) \n",
    "if all(env.action_space(i) == action_space and env.observation_space(i) == observation_space \n",
    "       for i in env.possible_agents):\n",
    "    print(f'They share the same observation {observation_space} and action {action_space} spaces')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a91ba-0043-4436-884c-670d67d87e2e",
   "metadata": {},
   "source": [
    "The `info` map returned by `reset(...)` and `step(...)` contains the action computed by original navground behavior, in this case `HL`, for each of the 10 agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9750f9c3-a422-42c3-b06c-02bbedb02865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation #0: {'ego_target_distance': array([1.3484901], dtype=float32), 'ego_target_direction': array([ 1.0000000e+00, -1.5725663e-08], dtype=float32), 'position': array([[-0.00738173, -0.30817246],\n",
      "       [-0.38925827,  0.01894906],\n",
      "       [-0.46368217, -0.4778133 ],\n",
      "       [ 0.15306982, -0.6674728 ],\n",
      "       [ 0.5088892 , -0.62434775]], dtype=float32), 'radius': array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float32), 'valid': array([1, 1, 1, 1, 1], dtype=uint8), 'velocity': array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)}\n",
      "Info #0: {'navground_action': array([0.32967997, 0.        ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "print(f\"Observation #0: {observations[0]}\")\n",
    "print(f\"Info #0: {infos[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d921c1f-6a65-4bc0-8ff9-e0e771a5e53e",
   "metadata": {},
   "source": [
    "Let's collect the reward from the original controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f46e603-500d-4d16-bd7d-b939209376f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 600 steps\n",
      "mean reward -0.243\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: info['navground_action'] for i, info in infos.items()}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5127035-c9df-4345-86a1-3463eabb05df",
   "metadata": {},
   "source": [
    "and compare it with the reward from a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b2a2b3-d5c3-4146-bb0d-ddebbc55d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 600 steps\n",
      "mean reward -1.117\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "all_rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: env.action_space(i).sample() for i in range(10)}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa6665-82bb-441f-b54f-1f0d25afded3",
   "metadata": {},
   "source": [
    "We want to use a machine learning policy to generate to action. For instance, a random policy, like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e144683-6316-4e64-83fd-781ed15cc006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground.learning.policies.random_predictor import RandomPredictor\n",
    "\n",
    "policies = {i: RandomPredictor(observation_space=env.observation_space(i), \n",
    "                               action_space=env.action_space(i)) \n",
    "            for i in env.agents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f31954-63c4-4fa9-82c8-a27611037230",
   "metadata": {},
   "source": [
    "Policies output a tuple `(action, state)`. Therefore the new loop is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303b7779-e50b-441b-9554-66a72adfaee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset after 600 steps\n",
      "mean reward -1.088\n"
     ]
    }
   ],
   "source": [
    "observations, infos = env.reset()\n",
    "rewards = []\n",
    "for n in range(1000):\n",
    "    actions = {i: policies[i].predict(observations[i])[0] for i in env.agents}\n",
    "    observations, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    all_rewards.append(np.mean(list(rewards.values())))\n",
    "    done = np.bitwise_or(list(terminated.values()), list(truncated.values()))\n",
    "    if np.all(done):\n",
    "        print(f'reset after {n} steps')\n",
    "        observations, infos = env.reset()\n",
    "\n",
    "print(f'mean reward {np.mean(all_rewards):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49992c5-64da-4860-9bb3-a7ba9a203e09",
   "metadata": {},
   "source": [
    "## Two groups\n",
    "\n",
    "Let us now consider the more complex case where we want to control agents using different sensors and/or configurations.\n",
    "For instance, we want to control the first 10 agents like before and the second 10 agents using a lidar scanner.\n",
    "Let say we also want to control the second group in acceleration vs the first group in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d760369a-cf39-47ba-ad8a-cb8272c6340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar = sim.load_state_estimation(\"\"\"\n",
    "type: Lidar\n",
    "resolution: 100\n",
    "range: 5.0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "121f6736-2017-4e8c-a5fd-35a3dc212ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground.learning.parallel_env import parallel_env\n",
    "from navground.learning import GroupConfig\n",
    "\n",
    "first_group = GroupConfig(indices=slice(0, 10, 1), sensor=sensor, \n",
    "                          observation = DefaultObservationConfig(include_target_distance=False), \n",
    "                          action = ControlActionConfig(), \n",
    "                          reward=SocialReward())\n",
    "second_group = GroupConfig(indices=slice(10, 20, 1), sensor=lidar, \n",
    "                           observation = DefaultObservationConfig(), \n",
    "                           action = ControlActionConfig(use_acceleration_action=True, \n",
    "                                                        max_acceleration=1.0, \n",
    "                                                        max_angular_acceleration=10.0), \n",
    "                           reward=SocialReward())\n",
    "\n",
    "env = parallel_env(scenario=scenario, groups=[first_group, second_group], \n",
    "                   time_step=0.1, max_duration=60.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff36a0-f985-43a9-af3c-13e3b530f545",
   "metadata": {},
   "source": [
    "The two groups uses now different observation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae72065e-0234-48e8-98e4-fabb0ac055a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('position': Box(-5.0, 5.0, (5, 2), float32), 'radius': Box(0.0, 0.1, (5,), float32), 'valid': Box(0, 1, (5,), uint8), 'velocity': Box(-0.12, 0.12, (5, 2), float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ce4ff0-6159-4983-9bec-df4da1a6d491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('fov': Box(0.0, 6.2831855, (1,), float32), 'range': Box(0.0, 5.0, (100,), float32), 'start_angle': Box(-6.2831855, 6.2831855, (1,), float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8adab-148f-4aac-bfe5-2135628cc506",
   "metadata": {},
   "source": [
    "and differnet maps between actions and commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a05288-55ad-4708-9635-20511c5d637f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twist2((0.120000, 0.000000), 2.553191, frame=Frame.relative)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._possible_agents[0].gym.get_cmd_from_action(np.ones(2), time_step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d27102a-fd11-45f2-bb80-24f3c3aa3963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twist2((0.100000, 0.000000), 1.000000, frame=Frame.relative)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._possible_agents[10].gym.get_cmd_from_action(np.ones(2), time_step=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cde0d0-ee7c-4d72-8bc3-c0f4d564a33f",
   "metadata": {},
   "source": [
    "## Convert to a Gymnasium Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5bfd0-c46e-4417-9c2d-2c1e834bfb57",
   "metadata": {},
   "source": [
    "In case the agents share the same configuration (and in particular action and observation spaces), we can convert the PettingZoo env in a Gymnasium vector env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ecbf625-7ba7-48a7-b713-a105c5d7bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = shared_parallel_env(\n",
    "    scenario=scenario,\n",
    "    agent_indices=slice(0, 10, 1),\n",
    "    sensor=sensor,\n",
    "    action=action_config,\n",
    "    observation=observation_config,\n",
    "    reward=SocialReward(),\n",
    "    time_step=0.1,\n",
    "    max_duration=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b2ce65-fadb-4810-b1d6-a7c55bd2efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supersuit\n",
    "\n",
    "venv = supersuit.pettingzoo_env_to_vec_env_v1(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d272656-4212-4519-b79b-2b134cab1222",
   "metadata": {},
   "source": [
    "with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a44418c4-86e2-4a70-aa15-2932ea9e230d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd81c8a-7cf5-4b87-b9e1-bb4665c64c86",
   "metadata": {},
   "source": [
    "environments that represents the individual agents. \n",
    "\n",
    "This vector env follows the Gymnasium API, stacking together observation, actions of the individual agents\n",
    "\n",
    "If we want instead an vector env to follows the SB3 API, we can use (even stacking multiple vectorized envs together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "080562c4-1d42-4bbc-aaf7-dda7c824f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "venv1 = supersuit.concat_vec_envs_v1(venv, 2, num_cpus=1, base_class=\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7f2bcd3-26ab-40f5-b885-2bb5630feae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv1.num_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa9541-f4a5-4b94-a5a4-113b1dcb8c5f",
   "metadata": {},
   "source": [
    "## Convert from a Gymnasium Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde282b-c921-4bb7-aa8e-29d5fcd3337d",
   "metadata": {},
   "source": [
    "If we have a single agent navground enviroment that uses a multi-agent scenario, we\n",
    "can convert it to a parallel environment, where all controlled agents share the same configuration, like for `shared_parallel_env`.\n",
    "\n",
    "Let us load the environment we saved in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad50d5a-ef97-4b46-a8fa-f19a0c608685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground.learning import io\n",
    "\n",
    "sa_env = io.load_env('env.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2001df-f6d2-4750-8e8d-d32a2bdb4aaf",
   "metadata": {},
   "source": [
    "and covert it to a parallel environment, controlling 10 (out of the total 20) agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57f69eb-3bc5-4b50-ba99-bb2cd5e0b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navground.learning.parallel_env import make_shared_parallel_env_with_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76cbef3f-7026-432f-914c-f41c2eb9ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = make_shared_parallel_env_with_env(env=sa_env, indices=slice(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23789c2d-3869-41ef-bec1-2a9a12ce4b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.possible_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d0310-bba7-4a19-a818-cac47eed9097",
   "metadata": {},
   "source": [
    "## Saving and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a67573-69d2-41f1-9d28-0a1e15987362",
   "metadata": {},
   "source": [
    "The multi-agent PettingZoo environment supports the same YAML representation like the single-agent Gymnasium environment and we can save it and load it from a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773946ed-0b7a-4591-a961-75c695fe412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "io.save_env(env1, 'penv.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb51d1-1a45-4926-b4e8-8bf987d112b8",
   "metadata": {},
   "source": [
    "Let us check that the ``groups`` field is coherent with the configuration we have just provided: a single group of 10 agents (indices 0, 1, ..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d6f651-f727-46cc-86c8-106ea0a544c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- action:\n",
      "    dof: null\n",
      "    dtype: ''\n",
      "    fix_orientation: false\n",
      "    has_wheels: null\n",
      "    max_acceleration: .inf\n",
      "    max_angular_acceleration: .inf\n",
      "    max_angular_speed: .inf\n",
      "    max_speed: .inf\n",
      "    type: Control\n",
      "    use_acceleration_action: false\n",
      "    use_wheels: false\n",
      "  indices:\n",
      "    start: 0\n",
      "    step: null\n",
      "    stop: 10\n",
      "    type: slice\n",
      "  observation:\n",
      "    dof: null\n",
      "    dtype: ''\n",
      "    flat: false\n",
      "    history: 1\n",
      "    include_angular_speed: false\n",
      "    include_radius: false\n",
      "    include_target_angular_speed: false\n",
      "    include_target_direction: true\n",
      "    include_target_direction_validity: false\n",
      "    include_target_distance: true\n",
      "    include_target_distance_validity: false\n",
      "    include_target_speed: false\n",
      "    include_velocity: false\n",
      "    max_angular_speed: .inf\n",
      "    max_radius: .inf\n",
      "    max_speed: .inf\n",
      "    max_target_distance: .inf\n",
      "    type: Default\n",
      "  reward:\n",
      "    alpha: 0.0\n",
      "    beta: 1.0\n",
      "    critical_safety_margin: 0.0\n",
      "    default_social_margin: 0.0\n",
      "    safety_margin: null\n",
      "    social_margins: {}\n",
      "    type: Social\n",
      "  sensor: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "print(yaml.safe_dump(env1.asdict['groups']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52478c6c-bddc-4a6a-b10c-19d8185a3f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
